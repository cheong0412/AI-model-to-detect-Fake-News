# 6.19 ì–‘ë°©í–¥ ë‹¤ì¸µ ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬
import torch
from torch import nn


input_size = 128
output_size = 256
num_layers = 3
bidirectional = True
proj_size = 64

model = nn.LSTM(
    input_size=input_size,
    hidden_size=output_size,
    num_layers=num_layers,
    batch_first=True,
    bidirectional=bidirectional,
    proj_size=proj_size,
)

batch_size = 4
sequence_len = 6

inputs = torch.randn(batch_size, sequence_len, input_size)
h_0 = torch.rand(
    num_layers * (int(bidirectional) + 1),
    batch_size,
    proj_size if proj_size > 0 else output_size,
)
c_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size)

outputs, (h_n, c_n) = model(inputs, (h_0, c_0))

print(outputs.shape)
print(h_n.shape)
print(c_n.shape)

# -------------------------------------------------------------------------------------------------------

# 6.20 ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸
from torch import nn


class SentenceClassifier(nn.Module):
    def __init__(
        self,
        n_vocab,
        hidden_dim,
        embedding_dim,
        n_layers,
        dropout=0.6,
        bidirectional=True,
        model_type="lstm"
    ):
        super().__init__()

        self.embedding = nn.Embedding(
            num_embeddings=n_vocab,
            embedding_dim=embedding_dim,
            padding_idx=0
        )
        if model_type == "rnn":
            self.model = nn.RNN(
                input_size=embedding_dim,
                hidden_size=hidden_dim,
                num_layers=n_layers,
                bidirectional=bidirectional,
                dropout=dropout,
                batch_first=True,
            )
        elif model_type == "lstm":
            self.model = nn.LSTM(
                input_size=embedding_dim,
                hidden_size=hidden_dim,
                num_layers=n_layers,
                bidirectional=bidirectional,
                dropout=dropout,
                batch_first=True,
            )

        if bidirectional:
            self.classifier = nn.Linear(hidden_dim * 2, 1)
        else:
            self.classifier = nn.Linear(hidden_dim, 1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, inputs):
        embeddings = self.embedding(inputs)
        output, _ = self.model(embeddings)
        last_output = output[:, -1, :]
        last_output = self.dropout(last_output)
        logits = self.classifier(last_output)
        return logits

# -------------------------------------------------------------------------------------------------------

# df_train = pd.read_csv('./fakenews_preprocessed_trian2_.csv')
# test_df = pd.read_csv('./fakenews_preprocessed_test2_.csv')

# train = df_train.sample(frac=0.9, random_state=42)
# test = test_df


import pandas as pd

train = pd.read_csv('./fakenews_preprocessed_trian2_.csv').sample(frac=0.9, random_state=42)
test = pd.read_csv('./fakenews_preprocessed_test2_.csv')


# -------------------------------------------------------------------------------------------------------

# 6.22 ë°ì´í„° í† í°í™” ë° ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶•
import pickle
import ast
from collections import Counter

def build_vocab(corpus, n_vocab, special_tokens, save_path=None, save_path_txt=None):
    counter = Counter()
    for i, tokens in enumerate(corpus):
        counter.update(tokens)
        if (i + 1) % 10000 == 0:
            print(f"{i + 1}ê°œì˜ ë¬¸ì¥ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    vocab = special_tokens[:]
    for token, count in counter.most_common(n_vocab):
        vocab.append(token)

    if save_path:
        with open(save_path, "wb") as f:
            pickle.dump(vocab, f)
        print(f"âœ… Vocab ì €ì¥ ì™„ë£Œ (pickle): {save_path}")

    if save_path_txt:
        with open(save_path_txt, "w", encoding="utf-8") as f:
            for token in vocab:
                f.write(f"{token}\n")
        print(f"âœ… Vocab ì €ì¥ ì™„ë£Œ (txt): {save_path_txt}")

    print(f"ğŸ“¦ ìµœì¢… vocab í¬ê¸°: {len(vocab)}ê°œ")
    return vocab


# content ì»¬ëŸ¼ì˜ ë¬¸ìì—´ì„ ì§„ì§œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
train_tokens = [ast.literal_eval(line) for line in train['content']]
test_tokens = [ast.literal_eval(line) for line in test['content']]

#vocab = build_vocab(corpus=train_tokens, n_vocab=80000, special_tokens=["<PAD>", "<UNK>"], save_path='vocab2.pkl', save_path_txt='vocab2.txt')

# print(vocab[:10])
# print(len(vocab))


# #
import os

VOCAB_PATH = "vocab2.pkl"

if os.path.exists(VOCAB_PATH):
    print("ğŸ“¥ ì €ì¥ëœ vocab ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    with open(VOCAB_PATH, "rb") as f:
        vocab = pickle.load(f)
else:
    print("ğŸ› ï¸ vocab ìƒˆë¡œ ìƒì„± ì¤‘...")
    vocab = build_vocab(
        corpus=train_tokens,
        n_vocab=80000,
        special_tokens=["<PAD>", "<UNK>"],
        save_path=VOCAB_PATH,
        save_path_txt="vocab.txt"
    )

token_to_id = {token: idx for idx, token in enumerate(vocab)}
id_to_token = {idx: token for idx, token in enumerate(vocab)}

# -------------------------------------------------------------------------------------------------------

# 6.23 ì •ìˆ˜ ì¸ì½”ë”© ë° íŒ¨ë”©
import numpy as np


def pad_sequences(sequences, max_length, pad_value):
    result = list()
    for sequence in sequences:
        sequence = sequence[:max_length]
        pad_length = max_length - len(sequence)
        padded_sequence = sequence + [pad_value] * pad_length
        result.append(padded_sequence)
    return np.asarray(result)


unk_id = token_to_id["<UNK>"]
train_ids = [
    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens
]
test_ids = [
    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens
]

max_length = 32
pad_id = token_to_id["<PAD>"]
train_ids = pad_sequences(train_ids, max_length, pad_id)
test_ids = pad_sequences(test_ids, max_length, pad_id)

print(train_ids[0])
print(test_ids[0])


# -------------------------------------------------------------------------------------------------------

# 6.24 ë°ì´í„°ë¡œë” ì ìš©
import torch
from torch.utils.data import TensorDataset, DataLoader


train_ids = torch.tensor(train_ids)
test_ids = torch.tensor(test_ids)

train_labels = torch.tensor(train.label.values, dtype=torch.float64)
test_labels = torch.tensor(test.label.values, dtype=torch.float64)

train_dataset = TensorDataset(train_ids, train_labels)
test_dataset = TensorDataset(test_ids, test_labels)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

print(f"Train dataset size: {len(train_loader.dataset)}")
print(f"Test dataset size: {len(test_loader.dataset)}")

# -------------------------------------------------------------------------------------------------------

# 6.25 ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” í•¨ìˆ˜ ì •ì˜
from torch import optim

n_vocab = len(token_to_id)
hidden_dim = 64
embedding_dim = 128
n_layers = 2

device = "cuda" if torch.cuda.is_available() else "cpu"
classifier = SentenceClassifier(
    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers
).to(device)
criterion = nn.BCEWithLogitsLoss().to(device)
optimizer = optim.RMSprop(classifier.parameters(), lr=0.001, weight_decay=1e-5) # weight_decay=1e-5  -> ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ íŒŒë¼ë¯¸í„° í¬ê¸°ë¥¼ ì œí•œí•˜ëŠ” ì½”ë“œ ì¶”ê°€í•¨.

# -------------------------------------------------------------------------------------------------------

# 6.26 ëª¨ë¸ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ -> pt ì €ì¥ í•„ìš” (ì—í¬í¬ ì•½ 10)
def train(model, datasets, criterion, optimizer, device, interval=100):
    model.train()
    losses = list()
    
    corrects = 0  # ì •í™•ë„ ê³„ì‚°ì„ ìœ„í•œ ì´ˆê¸°í™”
    total = 0     # ì´ ìƒ˜í”Œ ê°œìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ ì´ˆê¸°í™”

    for step, (input_ids, labels) in enumerate(datasets):
        input_ids = input_ids.to(device)
        labels = labels.to(device).unsqueeze(1)

        logits = model(input_ids)
        loss = criterion(logits, labels)
        losses.append(loss.item())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ì˜ˆì¸¡ê°’ êµ¬í•˜ê¸°
        yhat = torch.sigmoid(logits) > 0.5  # ì´ì§„ ë¶„ë¥˜ë¼ë©´ sigmoid ì‚¬ìš©

        # ì •í™•ë„ ê³„ì‚°
        corrects += torch.sum(yhat == labels).item()  # ë§ì¶˜ ì˜ˆì¸¡ì˜ ê°œìˆ˜ë¥¼ ëˆ„ì 
        total += labels.size(0)  # ì´ ìƒ˜í”Œ ê°œìˆ˜

        # 100ë²ˆë§ˆë‹¤ ì¶œë ¥
        if step % interval == 0:
            accuracy = corrects / total  # ì •í™•ë„ ê³„ì‚°
            print(f"Step {step}, Train Loss: {np.mean(losses)}, Train Accuracy: {accuracy * 100:.2f}%")

    print(f"Finished epoch with total {len(datasets)} samples.")
    return model  # ëª¨ë¸ ë°˜í™˜


def test(model, datasets, criterion, device):
    model.eval()
    losses = list()
    corrects = list()

    for step, (input_ids, labels) in enumerate(datasets):
        input_ids = input_ids.to(device)
        labels = labels.to(device).unsqueeze(1)

        logits = model(input_ids)
        loss = criterion(logits, labels)
        losses.append(loss.item())

        yhat = torch.sigmoid(logits) > 0.5
        corrects.extend(torch.eq(yhat, labels).cpu().tolist())

    val_loss = np.mean(losses)
    val_accuracy = np.mean(corrects)
    print(f"Val Loss : {val_loss}, Val Accuracy : {val_accuracy}")
    return val_accuracy


######## ì—í¬í¬ ë°˜ë³µ ë° ëª¨ë¸ ì €ì¥ ########
epochs = 10
interval = 100

for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")
    
    classifier.train()
    losses = []
    corrects = 0
    total = 0
    processed_samples = 0  # ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜ë¥¼ ì¶”ì í•˜ëŠ” ë³€ìˆ˜

    for step, (input_ids, labels) in enumerate(train_loader):
        input_ids = input_ids.to(device)
        labels = labels.to(device).unsqueeze(1)

        logits = classifier(input_ids)
        loss = criterion(logits, labels)
        losses.append(loss.item())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ì˜ˆì¸¡ê°’ êµ¬í•˜ê¸°
        yhat = torch.sigmoid(logits) > 0.5  # ì´ì§„ ë¶„ë¥˜ë¼ë©´ sigmoid ì‚¬ìš©

        # ì •í™•ë„ ê³„ì‚°
        corrects += torch.sum(yhat == labels).item()
        total += labels.size(0)

        # ë°°ì¹˜ë§ˆë‹¤ ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜ ì¶”ì 
        processed_samples += labels.size(0)

        # 100ë²ˆë§ˆë‹¤ ì¶œë ¥
        if step % interval == 0:
            accuracy = corrects / total  # ì •í™•ë„ ê³„ì‚°
            print(f"Step {step}, Train Loss: {np.mean(losses)}, Train Accuracy: {accuracy * 100:.2f}%")

    # ì—í¬í¬ ëë‚  ë•Œ ì²˜ë¦¬ëœ ì´ ë°ì´í„° ìˆ˜ ì¶œë ¥
    print(f"Epoch {epoch+1} finished. Total samples processed: {processed_samples}/{len(train_loader.dataset)}")

    # ëª¨ë¸ í…ŒìŠ¤íŠ¸ í›„ ì •í™•ë„ ê³„ì‚°
    accuracy = test(classifier, test_loader, criterion, device)

    # ëª¨ë¸ ì €ì¥ (.pt íŒŒì¼, íŒŒì¼ëª…ì— ì—í¬í¬ ë²ˆí˜¸ì™€ ì •í™•ë„ í¬í•¨)
    save_path = f"model_epoch_{epoch+1}_accuracy_{accuracy:.4f}.pt"
    torch.save(classifier.state_dict(), save_path)
    print(f"Model saved to {save_path}")

# -------------------------------------------------------------------------------------------------------